{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing: Loading in reviews data and process with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import pandas and set display option\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##load Amazon reviews dataset\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping empty cells and dupes \n",
    "text_summary = df.filter(items =['Summary','Text'])\n",
    "text_summary = text_summary.dropna()\n",
    "text_summary = text_summary.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data cleaning to remove unwanted symbols\n",
    "import re\n",
    "def clean_text(sentences):\n",
    "    clean=[]\n",
    "    \n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''    \n",
    "    for sentence in sentences:\n",
    "        for key in contractions.keys():\n",
    "            sentence = sentence.lower().replace(key, contractions[key])\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "        sentence = re.sub(r'\\<a href', ' ', sentence)\n",
    "        sentence = re.sub(r'&amp;', '', sentence) \n",
    "        sentence = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', sentence)\n",
    "        sentence = re.sub(r'<br />', ' ', sentence)\n",
    "        sentence = re.sub(r'\\'', ' ', sentence)\n",
    "        \n",
    "        words = [word for word in sentence.split()]\n",
    "        clean.append(words)\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the summaries and texts\n",
    "Texts = clean_text(text_summary[\"Text\"])\n",
    "Summaries = clean_text(text_summary[\"Summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product',\n",
       " 'arrived',\n",
       " 'labeled',\n",
       " 'as',\n",
       " 'jumbo',\n",
       " 'salted',\n",
       " 'peanuts',\n",
       " 'the',\n",
       " 'peanuts',\n",
       " 'were',\n",
       " 'actually',\n",
       " 'small',\n",
       " 'sized',\n",
       " 'unsalted',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'if',\n",
       " 'this',\n",
       " 'was',\n",
       " 'an',\n",
       " 'error',\n",
       " 'or',\n",
       " 'if',\n",
       " 'the',\n",
       " 'vendor',\n",
       " 'intended',\n",
       " 'to',\n",
       " 'represent',\n",
       " 'the',\n",
       " 'product',\n",
       " 'as',\n",
       " 'jumbo']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verify that one entry of the clearned data\n",
    "Texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to count word frequency\n",
    "def count_words(count_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence:\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 132873\n"
     ]
    }
   ],
   "source": [
    "## Apply functino above to generate the word frequency for each word and get vocab size in dataset\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, Summaries)\n",
    "count_words(word_counts, Texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understand how long summaries and texts generally are, so that we can exclude outliers later\n",
    "def create_lengths(text):\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "         counts\n",
      "count 394967.00\n",
      "mean       4.16\n",
      "std        2.65\n",
      "min        0.00\n",
      "25%        2.00\n",
      "50%        4.00\n",
      "75%        5.00\n",
      "max       48.00\n",
      "\n",
      "Texts:\n",
      "         counts\n",
      "count 394967.00\n",
      "mean      80.58\n",
      "std       77.55\n",
      "min        0.00\n",
      "25%       34.00\n",
      "50%       57.00\n",
      "75%       99.00\n",
      "max     3540.00\n"
     ]
    }
   ],
   "source": [
    "##printing for verification\n",
    "lengths_summaries = create_lengths(Summaries)\n",
    "lengths_texts = create_lengths(Texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wget\n",
    "# wget.download(\"http://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "# import zipfile\n",
    "# zf=zipfile.ZipFile(\"glove.6B.zip\")\n",
    "# zf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe\n",
    "#Can select from 50, 100, 200 and 300 for dimensions\n",
    "import numpy as np\n",
    "def parse_glove(dimension):\n",
    "    filename = \"glove.6B.{:d}d.txt\".format(dimension)\n",
    "    embeddings_index={}\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row=line.strip().split(' ')\n",
    "        word = row[0]\n",
    "        embedding = np.asarray(row[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "    print('Word embeddings:', len(embeddings_index))\n",
    "    file.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 400000\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 200 #set default embedding dimension to 200 \n",
    "embeddings_index = parse_glove(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from GloVe: 4171\n",
      "Percent of words that are missing|: 3.1399999999999997%\n"
     ]
    }
   ],
   "source": [
    "## Find the number of words that are missing from our data from GloVe, and appears more often than our set threshold\n",
    "missing_words = 0\n",
    "threshold = 10\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from GloVe:\", missing_words)\n",
    "print(\"Percent of words that are missing|: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 132873\n",
      "Number of words we will use: 65342\n"
     ]
    }
   ],
   "source": [
    "## create mapping to map each word to an integer ID\n",
    "word_to_id = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        word_to_id[word] = value\n",
    "        value += 1\n",
    "\n",
    "## Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "## Add tokens to vocab\n",
    "for code in codes:\n",
    "    word_to_id[code] = len(word_to_id)\n",
    "\n",
    "## Create mappingn for ID to word\n",
    "id_to_word = {}\n",
    "for word, value in word_to_id.items():\n",
    "    id_to_word[value] = word\n",
    "\n",
    "\n",
    "print(\"Number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to find nearest neighbor for unknown words\n",
    "W = list(embeddings_index.values())\n",
    "vocab = list(embeddings_index.keys())\n",
    "def nearest_neighbor(word):\n",
    "    v= embeddings_index[word]\n",
    "    dot_product = np.dot(W,v)\n",
    "    A = np.sqrt(np.sum(np.square(v),0))\n",
    "    B = np.sqrt(np.sum(np.square(W),1))\n",
    "    denominator = np.multiply(A,B)\n",
    "    cosine_similarities = np.divide(dot_product,denominator)\n",
    "    v_sim = W[np.argmax(cosine_similarities)]\n",
    "    for x in range(0,len(W)):\n",
    "        if np.array_equal(W[x],np.asarray(v_sim)):\n",
    "            return vocab[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Createa a reduced embedding dictionary\n",
    "word_embedding_matrix = np.zeros((len(word_to_id), embedding_dim), dtype=np.float32)\n",
    "for word, i in word_to_id.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in GloVe, we start with a randomized embedding\n",
    "        rand_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = rand_embedding\n",
    "        word_embedding_matrix[i] = rand_embedding\n",
    "## We also attempted to use the nearest neighbor for unknown words (function above)\n",
    "## We found this reduces speed rather significantly, and given the low % of NNs, we proceeded with the randomized embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert words to IDs (we do not convert directly to embeddings to increase the performance)\n",
    "def convert_to_ids(text, word_n, unk_n, eos=False): \n",
    "    ids = []\n",
    "    for sentence in text:\n",
    "        sentence_ids = []\n",
    "        for word in sentence:\n",
    "            word_n += 1\n",
    "            if word in word_to_id:\n",
    "                sentence_ids.append(word_to_id[word])\n",
    "            else:\n",
    "                sentence_ids.append(word_to_id[\"<UNK>\"])\n",
    "                unk_n += 1\n",
    "        if eos:\n",
    "            sentence_ids.append(word_to_id[\"<EOS>\"])\n",
    "        ids.append(sentence_ids)\n",
    "    return ids, word_n, unk_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of UNK: 0.35000000000000003%\n"
     ]
    }
   ],
   "source": [
    "## Apply the convert_to_ids function\n",
    "word_n = 0\n",
    "unk_n = 0\n",
    "\n",
    "id_summaries, word_n, unk_n_sum = convert_to_ids(Summaries, word_n, unk_n)\n",
    "id_texts, word_n, unk_n_text = convert_to_ids(Texts, word_n, unk_n, eos=True)\n",
    "unk_percent = round((unk_n_sum+unk_n_text)/word_n,4)*100\n",
    "\n",
    "print(\"Percent of UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unk(text):\n",
    "    unk_n = text.count(word_to_id[\"<UNK>\"])\n",
    "    return unk_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251405\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences by length to reduce padding\n",
    "sorted_id_summaries = []\n",
    "sorted_id_texts = []\n",
    "max_text_length = 100 #75% percentile\n",
    "min_summary_length = 2 #want to produce summaries more than 2 words\n",
    "min_text_length = 10 #ensure meaningful text worth summarizing\n",
    "max_unk_text = 2\n",
    "max_unk_summary = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for i, words in enumerate(id_summaries):\n",
    "        if (len(id_summaries[i]) >= min_summary_length and\n",
    "            len(id_texts[i])>= min_text_length and\n",
    "            len(id_texts[i])<= max_text_length and\n",
    "            count_unk(id_summaries[i])<=max_unk_summary and\n",
    "            count_unk(id_texts[i])<=max_unk_text and\n",
    "            len(id_texts[i]) == length):\n",
    "            \n",
    "            sorted_id_summaries.append(id_summaries[i])\n",
    "            sorted_id_texts.append(id_texts[i])\n",
    "            \n",
    "print(len(sorted_id_summaries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the convered IDs for texts and summaries so we don't have to rerun data processing again\n",
    "np.save(\"id_summaries\",sorted_id_summaries)\n",
    "np.save(\"id_texts\",sorted_id_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use when loading saved data\n",
    "# id_summaries = np.load('id_summaries.npy')\n",
    "# id_texts = np.load('id_texts.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade tensorflow\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to create tf.placeholders for hyper parameters and model inputs\n",
    "def model_inputs():\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to process input for decoding input (adding <GO> to begining of batch)\n",
    "def process_decoding_input(target_data, word_to_id, batch_size):\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], word_to_id['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create encoding layer function - can choose between bi-direction or one direction LSTM\n",
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    if direction == 1:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "\n",
    "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                         input_keep_prob = keep_prob)\n",
    "\n",
    "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n",
    "                                                              rnn_inputs,\n",
    "                                                              sequence_length,\n",
    "                                                              dtype=tf.float32)\n",
    "\n",
    "            return enc_output, enc_state\n",
    "        \n",
    "        \n",
    "    if direction == 2:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                            input_keep_prob = keep_prob)\n",
    "\n",
    "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                            input_keep_prob = keep_prob)\n",
    "\n",
    "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                            cell_bw, \n",
    "                                                                            rnn_inputs,\n",
    "                                                                            sequence_length,\n",
    "                                                                            dtype=tf.float32)\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            return enc_output, enc_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training logits\n",
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)    \n",
    "    \n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ ,_  = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoding layer with attention (Bahdanau) for training and inference\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, word_to_id, keep_prob, batch_size, num_layers, direction):\n",
    "    \n",
    "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
    "        for layer in range(num_layers):\n",
    "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "                lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    with tf.name_scope(\"Attention_Wrapper\"):\n",
    "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
    "            \n",
    "        \n",
    "    initial_state = dec_cell.zero_state(batch_size,dtype=tf.float32)\n",
    "    initial_state = initial_state.clone(cell_state = enc_state)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,  \n",
    "                                                    word_to_id['<GO>'], \n",
    "                                                    word_to_id['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, word_to_id, batch_size, direction):\n",
    "\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob, direction)\n",
    "    \n",
    "    dec_input = process_decoding_input(target_data, word_to_id, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        word_to_id, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers, direction)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create padding so each batch has the same sentence length\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [word_to_id['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "epochs = 5 ## We initially wanted to run for 10+ epochs, but couldn't afford to do to extremely long traning time\n",
    "batch_size = 32\n",
    "rnn_size = 128\n",
    "num_layers = 1\n",
    "learning_rate = 0.008\n",
    "keep_probability = 0.8\n",
    "direction = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/davidhe/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /Users/davidhe/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(word_to_id)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      word_to_id,\n",
    "                                                      batch_size, \n",
    "                                                      direction)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"cost\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits,targets,masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a training subset\n",
    "start = 0\n",
    "end = start + 10000\n",
    "Summaries_short = id_summaries[start:end]\n",
    "Texts_short = id_texts[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/5 Batch   10/312 - Loss:  6.567, Seconds: 152.67\n",
      "Epoch   1/5 Batch   20/312 - Loss:  3.298, Seconds: 133.78\n",
      "Epoch   1/5 Batch   30/312 - Loss:  3.480, Seconds: 70.72\n",
      "Epoch   1/5 Batch   40/312 - Loss:  2.920, Seconds: 240.64\n",
      "Epoch   1/5 Batch   50/312 - Loss:  2.673, Seconds: 145.80\n",
      "Epoch   1/5 Batch   60/312 - Loss:  2.561, Seconds: 91.69\n",
      "Epoch   1/5 Batch   70/312 - Loss:  2.737, Seconds: 91.72\n",
      "Epoch   1/5 Batch   80/312 - Loss:  2.966, Seconds: 91.75\n",
      "Epoch   1/5 Batch   90/312 - Loss:  2.584, Seconds: 196.65\n",
      "Epoch   1/5 Batch  100/312 - Loss:  2.760, Seconds: 122.95\n",
      "Average loss for this update: 3.235\n",
      "New Record!\n",
      "Epoch   1/5 Batch  110/312 - Loss:  2.563, Seconds: 123.32\n",
      "Epoch   1/5 Batch  120/312 - Loss:  2.721, Seconds: 165.00\n",
      "Epoch   1/5 Batch  130/312 - Loss:  2.741, Seconds: 103.17\n",
      "Epoch   1/5 Batch  140/312 - Loss:  2.638, Seconds: 132.88\n",
      "Epoch   1/5 Batch  150/312 - Loss:  2.725, Seconds: 91.29\n",
      "Epoch   1/5 Batch  160/312 - Loss:  2.541, Seconds: 101.86\n",
      "Epoch   1/5 Batch  170/312 - Loss:  2.899, Seconds: 122.65\n",
      "Epoch   1/5 Batch  180/312 - Loss:  2.463, Seconds: 146.12\n",
      "Epoch   1/5 Batch  190/312 - Loss:  2.669, Seconds: 112.23\n",
      "Epoch   1/5 Batch  200/312 - Loss:  2.584, Seconds: 260.97\n",
      "Average loss for this update: 2.639\n",
      "New Record!\n",
      "Epoch   1/5 Batch  210/312 - Loss:  2.448, Seconds: 102.35\n",
      "Epoch   1/5 Batch  220/312 - Loss:  2.520, Seconds: 85.81\n",
      "Epoch   1/5 Batch  230/312 - Loss:  2.658, Seconds: 81.35\n",
      "Epoch   1/5 Batch  240/312 - Loss:  2.309, Seconds: 123.34\n",
      "Epoch   1/5 Batch  250/312 - Loss:  2.113, Seconds: 179.42\n",
      "Epoch   1/5 Batch  260/312 - Loss:  2.688, Seconds: 123.32\n",
      "Epoch   1/5 Batch  270/312 - Loss:  2.741, Seconds: 112.73\n",
      "Epoch   1/5 Batch  280/312 - Loss:  2.700, Seconds: 102.48\n",
      "Epoch   1/5 Batch  290/312 - Loss:  2.596, Seconds: 81.86\n",
      "Epoch   1/5 Batch  300/312 - Loss:  2.599, Seconds: 92.47\n",
      "Average loss for this update: 2.583\n",
      "New Record!\n",
      "Epoch   1/5 Batch  310/312 - Loss:  2.913, Seconds: 108.45\n",
      "Epoch   2/5 Batch   10/312 - Loss:  3.096, Seconds: 195.96\n",
      "Epoch   2/5 Batch   20/312 - Loss:  2.311, Seconds: 173.65\n",
      "Epoch   2/5 Batch   30/312 - Loss:  2.596, Seconds: 91.94\n",
      "Epoch   2/5 Batch   40/312 - Loss:  2.214, Seconds: 309.61\n",
      "Epoch   2/5 Batch   50/312 - Loss:  2.080, Seconds: 187.60\n",
      "Epoch   2/5 Batch   60/312 - Loss:  2.063, Seconds: 121.74\n",
      "Epoch   2/5 Batch   70/312 - Loss:  2.177, Seconds: 121.80\n",
      "Epoch   2/5 Batch   80/312 - Loss:  2.352, Seconds: 124.35\n",
      "Epoch   2/5 Batch   90/312 - Loss:  2.067, Seconds: 255.35\n",
      "Epoch   2/5 Batch  100/312 - Loss:  2.226, Seconds: 160.52\n",
      "Average loss for this update: 2.313\n",
      "New Record!\n",
      "Epoch   2/5 Batch  110/312 - Loss:  2.090, Seconds: 160.36\n",
      "Epoch   2/5 Batch  120/312 - Loss:  2.190, Seconds: 540.90\n",
      "Epoch   2/5 Batch  130/312 - Loss:  2.244, Seconds: 195.67\n",
      "Epoch   2/5 Batch  140/312 - Loss:  2.189, Seconds: 240.75\n",
      "Epoch   2/5 Batch  150/312 - Loss:  2.218, Seconds: 162.96\n",
      "Epoch   2/5 Batch  160/312 - Loss:  2.088, Seconds: 174.86\n",
      "Epoch   2/5 Batch  170/312 - Loss:  2.374, Seconds: 215.56\n",
      "Epoch   2/5 Batch  180/312 - Loss:  2.076, Seconds: 249.31\n",
      "Epoch   2/5 Batch  190/312 - Loss:  2.225, Seconds: 205.99\n",
      "Epoch   2/5 Batch  200/312 - Loss:  2.184, Seconds: 444.94\n",
      "Average loss for this update: 2.179\n",
      "New Record!\n",
      "Epoch   2/5 Batch  210/312 - Loss:  2.091, Seconds: 176.27\n",
      "Epoch   2/5 Batch  220/312 - Loss:  2.141, Seconds: 149.69\n",
      "Epoch   2/5 Batch  230/312 - Loss:  2.228, Seconds: 140.24\n",
      "Epoch   2/5 Batch  240/312 - Loss:  1.976, Seconds: 211.73\n",
      "Epoch   2/5 Batch  250/312 - Loss:  1.832, Seconds: 315.11\n",
      "Epoch   2/5 Batch  260/312 - Loss:  2.312, Seconds: 224.01\n",
      "Epoch   2/5 Batch  270/312 - Loss:  2.330, Seconds: 196.58\n",
      "Epoch   2/5 Batch  280/312 - Loss:  2.298, Seconds: 189.79\n",
      "Epoch   2/5 Batch  290/312 - Loss:  2.250, Seconds: 141.23\n",
      "Epoch   2/5 Batch  300/312 - Loss:  2.206, Seconds: 167.52\n",
      "Average loss for this update: 2.206\n",
      "No Improvement.\n",
      "Epoch   2/5 Batch  310/312 - Loss:  2.491, Seconds: 187.18\n",
      "Epoch   3/5 Batch   10/312 - Loss:  2.945, Seconds: 264.38\n",
      "Epoch   3/5 Batch   20/312 - Loss:  2.127, Seconds: 226.71\n",
      "Epoch   3/5 Batch   30/312 - Loss:  2.383, Seconds: 119.44\n",
      "Epoch   3/5 Batch   40/312 - Loss:  2.034, Seconds: 408.65\n",
      "Epoch   3/5 Batch   50/312 - Loss:  1.899, Seconds: 245.75\n",
      "Epoch   3/5 Batch   60/312 - Loss:  1.881, Seconds: 155.09\n",
      "Epoch   3/5 Batch   70/312 - Loss:  1.978, Seconds: 155.06\n",
      "Epoch   3/5 Batch   80/312 - Loss:  2.140, Seconds: 163.96\n",
      "Epoch   3/5 Batch   90/312 - Loss:  1.869, Seconds: 342.98\n",
      "Epoch   3/5 Batch  100/312 - Loss:  2.025, Seconds: 223.85\n",
      "Average loss for this update: 2.123\n",
      "New Record!\n",
      "Epoch   3/5 Batch  110/312 - Loss:  1.903, Seconds: 221.61\n",
      "Epoch   3/5 Batch  120/312 - Loss:  2.014, Seconds: 291.43\n",
      "Epoch   3/5 Batch  130/312 - Loss:  1.979, Seconds: 188.05\n",
      "Epoch   3/5 Batch  140/312 - Loss:  1.988, Seconds: 241.60\n",
      "Epoch   3/5 Batch  150/312 - Loss:  2.029, Seconds: 162.61\n",
      "Epoch   3/5 Batch  160/312 - Loss:  1.889, Seconds: 183.96\n",
      "Epoch   3/5 Batch  170/312 - Loss:  2.147, Seconds: 211.73\n",
      "Epoch   3/5 Batch  180/312 - Loss:  1.836, Seconds: 252.57\n",
      "Epoch   3/5 Batch  190/312 - Loss:  2.002, Seconds: 203.63\n",
      "Epoch   3/5 Batch  200/312 - Loss:  1.932, Seconds: 441.31\n",
      "Average loss for this update: 1.962\n",
      "New Record!\n",
      "Epoch   3/5 Batch  210/312 - Loss:  1.877, Seconds: 184.07\n",
      "Epoch   3/5 Batch  220/312 - Loss:  1.942, Seconds: 141.10\n",
      "Epoch   3/5 Batch  230/312 - Loss:  2.021, Seconds: 139.49\n",
      "Epoch   3/5 Batch  240/312 - Loss:  1.805, Seconds: 211.05\n",
      "Epoch   3/5 Batch  250/312 - Loss:  1.691, Seconds: 299.94\n",
      "Epoch   3/5 Batch  260/312 - Loss:  2.080, Seconds: 205.38\n",
      "Epoch   3/5 Batch  270/312 - Loss:  2.074, Seconds: 194.63\n",
      "Epoch   3/5 Batch  280/312 - Loss:  2.064, Seconds: 175.88\n",
      "Epoch   3/5 Batch  290/312 - Loss:  1.996, Seconds: 140.18\n",
      "Epoch   3/5 Batch  300/312 - Loss:  1.998, Seconds: 158.26\n",
      "Average loss for this update: 1.982\n",
      "No Improvement.\n",
      "Epoch   3/5 Batch  310/312 - Loss:  2.151, Seconds: 176.92\n",
      "Epoch   4/5 Batch   10/312 - Loss:  2.755, Seconds: 259.53\n",
      "Epoch   4/5 Batch   20/312 - Loss:  1.986, Seconds: 226.45\n",
      "Epoch   4/5 Batch   30/312 - Loss:  2.204, Seconds: 118.71\n",
      "Epoch   4/5 Batch   40/312 - Loss:  1.881, Seconds: 397.25\n",
      "Epoch   4/5 Batch   50/312 - Loss:  1.774, Seconds: 246.94\n",
      "Epoch   4/5 Batch   60/312 - Loss:  1.767, Seconds: 150.73\n",
      "Epoch   4/5 Batch   70/312 - Loss:  1.836, Seconds: 151.22\n",
      "Epoch   4/5 Batch   80/312 - Loss:  1.990, Seconds: 154.70\n",
      "Epoch   4/5 Batch   90/312 - Loss:  1.708, Seconds: 327.25\n",
      "Epoch   4/5 Batch  100/312 - Loss:  1.872, Seconds: 209.51\n",
      "Average loss for this update: 1.973\n",
      "No Improvement.\n",
      "Epoch   4/5 Batch  110/312 - Loss:  1.792, Seconds: 204.79\n",
      "Epoch   4/5 Batch  120/312 - Loss:  1.894, Seconds: 280.03\n",
      "Epoch   4/5 Batch  130/312 - Loss:  1.897, Seconds: 169.40\n",
      "Epoch   4/5 Batch  140/312 - Loss:  1.848, Seconds: 221.36\n",
      "Epoch   4/5 Batch  150/312 - Loss:  1.911, Seconds: 156.58\n",
      "Epoch   4/5 Batch  160/312 - Loss:  1.782, Seconds: 169.95\n",
      "Epoch   4/5 Batch  170/312 - Loss:  1.993, Seconds: 204.92\n",
      "Epoch   4/5 Batch  180/312 - Loss:  1.742, Seconds: 240.62\n",
      "Epoch   4/5 Batch  190/312 - Loss:  1.882, Seconds: 189.19\n",
      "Epoch   4/5 Batch  200/312 - Loss:  1.858, Seconds: 429.44\n",
      "Average loss for this update: 1.85\n",
      "New Record!\n",
      "Epoch   4/5 Batch  210/312 - Loss:  1.756, Seconds: 169.78\n",
      "Epoch   4/5 Batch  220/312 - Loss:  1.841, Seconds: 134.58\n",
      "Epoch   4/5 Batch  230/312 - Loss:  1.873, Seconds: 139.46\n",
      "Epoch   4/5 Batch  240/312 - Loss:  1.703, Seconds: 208.11\n",
      "Epoch   4/5 Batch  250/312 - Loss:  1.580, Seconds: 293.24\n",
      "Epoch   4/5 Batch  260/312 - Loss:  1.937, Seconds: 209.66\n",
      "Epoch   4/5 Batch  270/312 - Loss:  1.955, Seconds: 191.49\n",
      "Epoch   4/5 Batch  280/312 - Loss:  1.932, Seconds: 170.10\n",
      "Epoch   4/5 Batch  290/312 - Loss:  1.881, Seconds: 140.05\n",
      "Epoch   4/5 Batch  300/312 - Loss:  1.822, Seconds: 160.45\n",
      "Average loss for this update: 1.855\n",
      "No Improvement.\n",
      "Epoch   4/5 Batch  310/312 - Loss:  2.032, Seconds: 171.77\n",
      "Epoch   5/5 Batch   10/312 - Loss:  2.580, Seconds: 254.04\n",
      "Epoch   5/5 Batch   20/312 - Loss:  1.902, Seconds: 219.88\n",
      "Epoch   5/5 Batch   30/312 - Loss:  2.127, Seconds: 118.20\n",
      "Epoch   5/5 Batch   40/312 - Loss:  1.814, Seconds: 393.56\n",
      "Epoch   5/5 Batch   50/312 - Loss:  1.678, Seconds: 238.40\n",
      "Epoch   5/5 Batch   60/312 - Loss:  1.644, Seconds: 152.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/5 Batch   70/312 - Loss:  1.715, Seconds: 153.01\n",
      "Epoch   5/5 Batch   80/312 - Loss:  1.823, Seconds: 152.08\n",
      "Epoch   5/5 Batch   90/312 - Loss:  1.625, Seconds: 323.80\n",
      "Epoch   5/5 Batch  100/312 - Loss:  1.761, Seconds: 202.95\n",
      "Average loss for this update: 1.864\n",
      "No Improvement.\n",
      "Epoch   5/5 Batch  110/312 - Loss:  1.684, Seconds: 203.31\n",
      "Epoch   5/5 Batch  120/312 - Loss:  1.772, Seconds: 272.91\n",
      "Epoch   5/5 Batch  130/312 - Loss:  1.780, Seconds: 169.90\n",
      "Epoch   5/5 Batch  140/312 - Loss:  1.765, Seconds: 220.28\n",
      "Epoch   5/5 Batch  150/312 - Loss:  1.776, Seconds: 153.83\n",
      "Epoch   5/5 Batch  160/312 - Loss:  1.675, Seconds: 168.85\n",
      "Epoch   5/5 Batch  170/312 - Loss:  1.855, Seconds: 203.74\n",
      "Epoch   5/5 Batch  180/312 - Loss:  1.614, Seconds: 235.69\n",
      "Epoch   5/5 Batch  190/312 - Loss:  1.790, Seconds: 189.45\n",
      "Epoch   5/5 Batch  200/312 - Loss:  1.743, Seconds: 418.95\n",
      "Average loss for this update: 1.739\n",
      "New Record!\n",
      "Epoch   5/5 Batch  210/312 - Loss:  1.699, Seconds: 173.16\n",
      "Epoch   5/5 Batch  220/312 - Loss:  1.724, Seconds: 92.34\n",
      "Epoch   5/5 Batch  230/312 - Loss:  1.750, Seconds: 81.83\n",
      "Epoch   5/5 Batch  240/312 - Loss:  1.643, Seconds: 123.86\n",
      "Epoch   5/5 Batch  250/312 - Loss:  1.527, Seconds: 202.63\n",
      "Epoch   5/5 Batch  260/312 - Loss:  1.802, Seconds: 127.78\n",
      "Epoch   5/5 Batch  270/312 - Loss:  1.780, Seconds: 201.33\n",
      "Epoch   5/5 Batch  280/312 - Loss:  1.791, Seconds: 128.21\n",
      "Epoch   5/5 Batch  290/312 - Loss:  1.789, Seconds: 84.65\n",
      "Epoch   5/5 Batch  300/312 - Loss:  1.692, Seconds: 93.66\n",
      "Average loss for this update: 1.732\n",
      "New Record!\n",
      "Epoch   5/5 Batch  310/312 - Loss:  1.812, Seconds: 109.00\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 10 # Check training loss\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(Texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(Summaries_short, Texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(Texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('Improved Result') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    save_path = saver.save(sess, \"./Checkpoint/model.ckpt\")\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Checkpoint/model.ckpt\n",
      "Original Text: ['i', 'purchased', 'this', 'primarily', 'because', 'dr', 'oz', 'promoted', 'it', 'it', 'is', 'ok', 'but', 'tastes', 'just', 'like', 'the', 'cheap', 'puffed', 'rice', 'from', 'the', 'grocery', 'and', 'it', 'is', 'pricey', 'for', 'this', 'kamut', 'maybe', 'it', 'is', 'super', 'good', 'for', 'you', 'but', '6', 'oz', 'is', 'not', 'a', 'lot']\n",
      "Original Text: ['i', 'purchased', 'this', 'primarily', 'because', 'dr', 'oz', 'promoted', 'it', 'it', 'is', 'ok', 'but', 'tastes', 'just', 'like', 'the', 'cheap', 'puffed', 'rice', 'from', 'the', 'grocery', 'and', 'it', 'is', 'pricey', 'for', 'this', 'kamut', 'maybe', 'it', 'is', 'super', 'good', 'for', 'you', 'but', '6', 'oz', 'is', 'not', 'a', 'lot']\n",
      "  Input Words: i purchased this primarily because dr oz promoted it it is ok but tastes just like the cheap puffed rice from the grocery and it is pricey for this kamut maybe it is super good for you but 6 oz is not a lot\n",
      "\\Generated Summary\n",
      "  Response Words: not vegetarian\n"
     ]
    }
   ],
   "source": [
    "## Pull a random sentence from the unseen text in data and see the prediction\n",
    "random = np.random.randint(end,len(Texts))\n",
    "input_sentence = Texts[random]\n",
    "text = [word_to_id.get(word, word_to_id['<UNK>']) for word in input_sentence]\n",
    "\n",
    "checkpoint = \"./Checkpoint/model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from output\n",
    "pad = word_to_id[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "print('  Input Words: {}'.format(\" \".join([id_to_word[i] for i in text])))\n",
    "\n",
    "print('\\Generated Summary')\n",
    "print('  Response Words: {}'.format(\" \".join([id_to_word[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /Users/davidhe/anaconda/lib/python3.6/site-packages (0.3.1)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[31mtimestring 1.6.2 has requirement pytz==2013b, but you'll have pytz 2018.4 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install the library\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ho uhm\n",
      "not vegetarian\n"
     ]
    }
   ],
   "source": [
    "summary_sentence = Summaries[random]\n",
    "summary_text = [word_to_id.get(word, word_to_id['<UNK>']) for word in summary_sentence]\n",
    "original_summary = \" \".join([id_to_word[i] for i in summary_text])\n",
    "generated_summary = \" \".join([id_to_word[i] for i in answer_logits if i != pad])\n",
    "print(original_summary)\n",
    "print(generated_summary)\n",
    "# original_summary = \"this tea is my new alternative after jasmine green tea it is so smooth and light ican drink it anytime\"\n",
    "# generated_summary = \"great tea\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge_calculator = Rouge()\n",
    "scores = rouge_calculator.get_scores(original_summary, generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metric Name: rouge-1\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-score: 0.0\n",
      "\n",
      "Metric Name: rouge-2\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-score: 0.0\n",
      "\n",
      "Metric Name: rouge-l\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Pretty-print the results\n",
    "#note that this is just for ONE original and generated summary. \n",
    "for metric in scores:\n",
    "    for metric_name, metric_vals in metric.items():\n",
    "        print(\"\\nMetric Name: {}\\nPrecision: {}\\nRecall: {}\\nF-score: {}\".format(\n",
    "                metric_name, metric_vals['p'], metric_vals['r'], metric_vals['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE Metrics (Aggregate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select test texts and infer predicted summaries\n",
    "##size / number of summaries can be edited to get an idea of overall efficiency of set \n",
    "size = 100\n",
    "test_list = np.random.randint(10000,len(Texts),size = size)\n",
    "summary_tests = [Summaries[i] for i in test_list]\n",
    "text_tests = [Texts[i] for i in test_list]\n",
    "Predicted_Summaries = []\n",
    "checkpoint = \"./Checkpoint/model.ckpt\"\n",
    "pad = word_to_id[\"<PAD>\"] \n",
    "\n",
    "text_id_tests = []\n",
    "for text in text_tests:\n",
    "    ids = [word_to_id.get(word, word_to_id['<UNK>']) for word in text]\n",
    "    text_id_tests.append(ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_id_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Checkpoint/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    for text in text_id_tests:\n",
    "        if len(text) == 0:\n",
    "            Predicted_Summaries.append('')            \n",
    "        else:\n",
    "            answer = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                          summary_length: [np.random.randint(5,8)], \n",
    "                                          text_length: [len(text)]*batch_size,\n",
    "                                          keep_prob: 1.0})[0] \n",
    "            pred = [id_to_word[i] for i in answer if i != pad]\n",
    "            Predicted_Summaries.append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['great', 'coffee'],\n",
       " ['great', 'product'],\n",
       " ['great', 'tea'],\n",
       " ['great', 'pecans'],\n",
       " ['great', 'standby'],\n",
       " ['green', 'tea'],\n",
       " ['great', 'product'],\n",
       " ['great', 'pecans'],\n",
       " ['great', 'pecans'],\n",
       " ['great', 'tea'],\n",
       " ['great', 'snack'],\n",
       " ['these', 'are', 'wonderful'],\n",
       " ['abit', 'vegetable', 'beef', 'rinds'],\n",
       " ['great', 'product'],\n",
       " ['great', 'tasting'],\n",
       " ['great', 'product'],\n",
       " ['she', 'loved', 'it'],\n",
       " ['great', 'for', 'making'],\n",
       " ['works', 'well'],\n",
       " ['great', 'taste'],\n",
       " ['great', 'product'],\n",
       " ['my', 'dog', 'loves', 'this', 'stuff'],\n",
       " ['great', 'product'],\n",
       " ['great', 'tasting'],\n",
       " ['spearmint', 'gum'],\n",
       " ['pamela', 's', 'bread', 'mix'],\n",
       " ['great', 'product'],\n",
       " ['best', 'tasting', 'ever'],\n",
       " ['great', 'product'],\n",
       " ['great', 'product'],\n",
       " ['great', 'product'],\n",
       " ['great', 'standby'],\n",
       " ['great', 'product'],\n",
       " ['great', 'tea'],\n",
       " ['great', 'product'],\n",
       " ['great', 'product'],\n",
       " ['i', 'expected'],\n",
       " ['great', 'pecans'],\n",
       " ['cat', 'food'],\n",
       " ['cats', 'love', 'this', 'stuff'],\n",
       " ['worthless', 'the', 'bounty', 'candy'],\n",
       " ['great', 'coffee'],\n",
       " ['great', 'tea'],\n",
       " ['great', 'tea'],\n",
       " ['great', 'deal'],\n",
       " ['dogs', 'love', 'them'],\n",
       " ['great', 'standby'],\n",
       " ['best', 'tasting', 'tea'],\n",
       " ['great', 'pecans'],\n",
       " ['great', 'product'],\n",
       " ['great', 'pecans'],\n",
       " ['she', 'liked'],\n",
       " ['great', 'pecans'],\n",
       " ['great', 'product'],\n",
       " ['measurements', 'c', 'tangerine'],\n",
       " ['cats', 'love', 'this'],\n",
       " ['great', 'product'],\n",
       " ['great', 'product'],\n",
       " ['lemon', 'granules', 'raspberry', 'tea'],\n",
       " ['great', 'product'],\n",
       " ['cheapest', 'way'],\n",
       " ['great', 'product'],\n",
       " ['great', 'tea'],\n",
       " ['abit', 'mm', 'mm', 'mm', 'mm'],\n",
       " ['great', 'tea'],\n",
       " ['not', 'grav', 'lox'],\n",
       " ['great', 'tea'],\n",
       " ['chi', 'lemonhead'],\n",
       " ['great', 'taste'],\n",
       " ['great', 'product'],\n",
       " ['great', 'taste'],\n",
       " ['great', 'product'],\n",
       " ['great', 'product'],\n",
       " ['great', 'product'],\n",
       " ['van', 'hotte'],\n",
       " ['great', 'product'],\n",
       " ['great', 'standby'],\n",
       " ['great', 'pecans'],\n",
       " ['kona', 'cups'],\n",
       " ['great', 'gluten', 'free', 'bread'],\n",
       " ['great', 'deal'],\n",
       " ['great', 'product'],\n",
       " ['best', 'tasting', 'tea'],\n",
       " ['kona', 'blend'],\n",
       " ['not', 'grav', 'lox'],\n",
       " ['pamela', 's', 'bread', 'mix'],\n",
       " ['not', 'grav', 'lox'],\n",
       " ['complete', 'satisfaction'],\n",
       " ['my', 'favorite'],\n",
       " ['noirot', 'orange', 'flower', 'flower'],\n",
       " ['great', 'stuff'],\n",
       " ['great', 'product'],\n",
       " ['culinary', 'gift'],\n",
       " ['great', 'tea'],\n",
       " ['chai', 'chai', 'lover'],\n",
       " ['great', 'product'],\n",
       " ['worthless', 'the', 'best'],\n",
       " ['great', 'taste'],\n",
       " ['great', 'snack'],\n",
       " ['gluten', 'free', 'pantry']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predicted_Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_n = 1\n",
    "ngram_o = []\n",
    "for sent in summary_tests:\n",
    "    ngram_set_o = set()\n",
    "    text_length = len(sent)\n",
    "    max_index_ngram_start = text_length - rouge_n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set_o.add(tuple(sent[i:i + rouge_n]))\n",
    "    ngram_o.append(ngram_set_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_p = []\n",
    "for sent in Predicted_Summaries:\n",
    "    ngram_set_p = set()\n",
    "    text_length = len(sent)\n",
    "    max_index_ngram_start = text_length - rouge_n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set_p.add(tuple(sent[i:i + rouge_n]))\n",
    "    ngram_p.append(ngram_set_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for i in range(len(ngram_o)):\n",
    "    overlap = ngram_o[i].intersection(ngram_p[i])\n",
    "    overlap_n = len(overlap)\n",
    "    o_n = len(ngram_o[i])\n",
    "    p_n = len(ngram_p[i])\n",
    "    \n",
    "    if p_n == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlap_n / p_n\n",
    "\n",
    "    if o_n == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlap_n / o_n\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    \n",
    "    f1_scores.append(f1_score)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rouge_f1 = np.mean(f1_scores)\n",
    "avg_rouge_precision = np.mean(precisions)\n",
    "avg_rouge_recall = np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['used', 'to', 'love', 'now', 'disappointed'], ['good', 'but', 'still', 'having', 'gas'], ['cinnamon', 'way', 'too', 'strong'], ['flavorful', 'seeds'], ['a', 'nice', 'mellow', 'wash', 'of', 'turmeric'], ['sweet', 'and', 'spicy', 'red', 'tea'], ['beautiful', 'bonsai'], ['crunchy', 'tastes', 'great', 'is', 'healthy'], ['delicious', 'and', 'convenient'], ['perfect', 'when', 'you', 'have', 'a', 'cold'], ['do', 'you', 'like', 'sweet', 'potato', 'fries'], ['yum'], ['great', 'on', 'grilled', 'salmon'], ['my', 'son', 'loves', 'these'], ['secret', 'ingredient'], ['treats', 'for', 'a', 'treat~'], ['pina', 'colada'], ['quick', 'meal'], ['warning', 'contains', 'sucalose'], ['tasty'], ['intresting'], ['not', 'the', 'same', 'taste'], ['mmm', 'good'], ['horrible'], ['flavors', 'do', 'not', 'last'], ['busted', 'bags'], ['delicious', 'much', 'better', 'than', 'r', 'k'], ['great', 'for', 'cooking', 'or', 'straight', 'up'], ['great', 'south', 'african', 'spice'], ['best', 'price', 'i', 'can', 'find'], ['oh', 'i', 'did', 'not', 'know', 'these', 'were', 'still', 'around'], ['sicilian', 'oil', 'cured', 'black', 'olives'], ['first', 'food', 'my', 'dog', 'eats', 'consistently'], ['awesome'], ['more', 'than', 'soup'], ['amazing', 'pamela', 's', 'baking', 'mix'], ['good', 'but', 'subtle', 'chocolate', 'flavoring', 'recommended'], ['gel', 'ham', 'soup', 'base'], ['excellent', 'for', 'a', 'picky', 'cat'], ['they', 'eat', 'it'], ['favorite', 'christmas', 'present'], ['as', 'good', 'as', 'a', 'trip', 'to', 'starbucks'], ['my', 'favorite', 'herbal', 'tea', 'ever'], ['expensive', 'and', 'almost', 'worth', 'it'], ['tastes', 'like', 'home'], ['why', 'take', 'a', 'chance'], ['yummy'], ['good', 'pumpkin', 'flavor'], ['great', 'price', 'great', 'product'], ['good', 'but', 'could', 'be', 'better'], ['just', 'ok', 'smaller', 'servings', 'though'], ['a', 'natural', 'laxative', 'for', 'cats', 'perhaps'], ['mis', 'categorized', 'beware'], ['big', 'let', 'down'], ['cap', 'was', 'leaking'], ['my', 'cats', 'love', 'this', 'food'], ['it', 'is', 'great', 'stuff'], ['great', 'for', 'biking'], ['comparable', 'to', 'tazo', 'zen', 'tea', 'but', 'not', 'as', 'good'], ['the', 'name', 'says', 'it', 'all'], ['extremely', 'disappointed'], ['guilt', 'free', 'peanutbutter', 'who', 'knew'], ['curiously', 'unique'], ['not', 'the', 'same', 'as', 'the', 'filling', 'from', 'the', 'no', 'bake', 'cheesecake', 'box'], ['some', 'like', 'it', 'hot', 'cost', 'makes', 'it', 'not'], ['not', 'odor', 'free'], ['blissful', 'sighs'], ['sushi', 'chef', 'making', 'kit'], ['love', 'this', 'item', 'and', 'what', 'a', 'great', 'party', 'gift'], ['got', 'what', 'i', 'expected', 'delicious'], ['very', 'bad', 'after', 'taste', 'from', 'artifical', 'sweetener'], ['my', 'cat', 's', 'favorite', 'food'], ['over', 'priced'], ['navitas', 'goji', 'berries', 'made', 'in', 'china'], ['got', 'my', 'hubby', 's', 'approval'], ['edananes'], ['my', 'favorite', 'olive', 'oil'], ['i', 'use', 'it', 'for', 'baking', 'and', 'its', 'wonderful'], ['was', 'disappointed', 'in', 'the', 'texture', 'but', 'not', 'the', 'taste'], ['best', 'macaroni', 'ever'], ['15', 'a', 'bag'], ['great', 'taste'], ['great', 'flavor'], ['douwe', 'egberts', 'coffee'], ['leslie'], ['best', 'almond', 'flour'], ['disappointing'], ['great', 'seller'], ['my', 'son', 'loves', 'this', 'drink'], ['just', 'what', 'i', 'needed'], ['great', 'food', 'with', 'grrrrrrreat', 'taste'], ['convenient', 'delicious', 'and', 'cute'], ['aunt', 'was', 'surprised'], ['i', 'love', 'this', 'tea', 'lt', '3'], ['good', 'but', 'not', 'great'], ['great', 'cat', 'treat', 'at', 'wonderful', 'price'], ['wrong', 'size', 'jar'], ['great', 'tasting', 'but', 'primarily', 'corn', 'syrup'], ['always', 'delicious', 'and', 'always', 'expensive'], ['again', 'mis', 'sorted', 'into', 'gluten', 'free', 'and', 'it', 'is', 'not']]\n"
     ]
    }
   ],
   "source": [
    "print(summary_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great', 'coffee'], ['great', 'product'], ['great', 'tea'], ['great', 'pecans'], ['great', 'standby'], ['green', 'tea'], ['great', 'product'], ['great', 'pecans'], ['great', 'pecans'], ['great', 'tea'], ['great', 'snack'], ['these', 'are', 'wonderful'], ['abit', 'vegetable', 'beef', 'rinds'], ['great', 'product'], ['great', 'tasting'], ['great', 'product'], ['she', 'loved', 'it'], ['great', 'for', 'making'], ['works', 'well'], ['great', 'taste'], ['great', 'product'], ['my', 'dog', 'loves', 'this', 'stuff'], ['great', 'product'], ['great', 'tasting'], ['spearmint', 'gum'], ['pamela', 's', 'bread', 'mix'], ['great', 'product'], ['best', 'tasting', 'ever'], ['great', 'product'], ['great', 'product'], ['great', 'product'], ['great', 'standby'], ['great', 'product'], ['great', 'tea'], ['great', 'product'], ['great', 'product'], ['i', 'expected'], ['great', 'pecans'], ['cat', 'food'], ['cats', 'love', 'this', 'stuff'], ['worthless', 'the', 'bounty', 'candy'], ['great', 'coffee'], ['great', 'tea'], ['great', 'tea'], ['great', 'deal'], ['dogs', 'love', 'them'], ['great', 'standby'], ['best', 'tasting', 'tea'], ['great', 'pecans'], ['great', 'product'], ['great', 'pecans'], ['she', 'liked'], ['great', 'pecans'], ['great', 'product'], ['measurements', 'c', 'tangerine'], ['cats', 'love', 'this'], ['great', 'product'], ['great', 'product'], ['lemon', 'granules', 'raspberry', 'tea'], ['great', 'product'], ['cheapest', 'way'], ['great', 'product'], ['great', 'tea'], ['abit', 'mm', 'mm', 'mm', 'mm'], ['great', 'tea'], ['not', 'grav', 'lox'], ['great', 'tea'], ['chi', 'lemonhead'], ['great', 'taste'], ['great', 'product'], ['great', 'taste'], ['great', 'product'], ['great', 'product'], ['great', 'product'], ['van', 'hotte'], ['great', 'product'], ['great', 'standby'], ['great', 'pecans'], ['kona', 'cups'], ['great', 'gluten', 'free', 'bread'], ['great', 'deal'], ['great', 'product'], ['best', 'tasting', 'tea'], ['kona', 'blend'], ['not', 'grav', 'lox'], ['pamela', 's', 'bread', 'mix'], ['not', 'grav', 'lox'], ['complete', 'satisfaction'], ['my', 'favorite'], ['noirot', 'orange', 'flower', 'flower'], ['great', 'stuff'], ['great', 'product'], ['culinary', 'gift'], ['great', 'tea'], ['chai', 'chai', 'lover'], ['great', 'product'], ['worthless', 'the', 'best'], ['great', 'taste'], ['great', 'snack'], ['gluten', 'free', 'pantry']]\n"
     ]
    }
   ],
   "source": [
    "print(Predicted_Summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0637986449671\n",
      "0.1025\n",
      "0.0486507936508\n"
     ]
    }
   ],
   "source": [
    "#general average of selected number of generated summaries against corresponding summaries from data set\n",
    "print(avg_rouge_f1)\n",
    "print(avg_rouge_precision)\n",
    "print(avg_rouge_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "example_n = 5\n",
    "ex_summary = []\n",
    "ex_text = []\n",
    "pred_summary = []\n",
    "\n",
    "for i in summary_tests[:example_n]:\n",
    "    ex_summary.append(\" \".join(i))\n",
    "\n",
    "for j in text_tests[:example_n]:\n",
    "    ex_text.append(\" \".join(j))\n",
    "    \n",
    "for k in Predicted_Summaries[:example_n]:\n",
    "    pred_summary.append(\" \".join(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = pd.DataFrame({\n",
    "    'Summary': ex_summary,\n",
    "    'Predicted Summary': pred_summary,\n",
    "    'Text': ex_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Predicted Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>used to love now disappointed</td>\n",
       "      <td>great coffee</td>\n",
       "      <td>i used to love this decaf was my favorite great flavor non bitter but good flavor bordering on strong which i liked the last carton i received through amazon com is weak i can only use the small cup setting to make it okay i used to use the larger setting and add a little extra water has the amount of coffee in the k cups been reduced if the next batch is the same i will be looking for another brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good but still having gas</td>\n",
       "      <td>great product</td>\n",
       "      <td>my twins took this formula since they were born sometimes they having gas with this kind of formula sensitive i think they are no miracles formula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cinnamon way too strong</td>\n",
       "      <td>great tea</td>\n",
       "      <td>i love peach tea and anything peach flavored but this has has a cinnamon flavor that is too strong i wanted so badly to give it 5 stars but i would not be honest if i did i do not like the cinnamon flavor to me peach and cinnamon do not belong together peach is summer and cinnamon is winter this tea might be better in the winter as hot tea but then again not because peach is too summery cinnamon would go better with apple flavor as soon as i finish this box i will not buy it again &lt;br &gt;&lt;br &gt;the cinnamon is way too strong it made me sick at my stomach and the aftertaste is horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flavorful seeds</td>\n",
       "      <td>great pecans</td>\n",
       "      <td>i am absolutely thrilled to have 2 lbs of the frontier natural products whole cumin seed 16 ounce bags pack of 2 that i purchased directly from amazon they are delicious and flavorful since these are the whole seeds they should remain tasty for years i also have enough extra to make spice mixes for gifts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a nice mellow wash of turmeric</td>\n",
       "      <td>great standby</td>\n",
       "      <td>firstly i may be reviewing a slightly different nguan soon curry product than what is being sold i normally buy this at a wonderful thai owned asian market in lexington my bottle from nguan soon is the same as the picture except the labeling languages are slightly different i believe there is vietnamese on mine instead as it says b t curry right under curry powder after googling the word and translating from vietnamese to french to english it appears to mean flour or powder so that is my cautionary notice should you buy this powder be aware that my bottle lists as its ingredients the following turmeric coriander cumin chili salt and pepper it does not mention garlic at all on my bottle but looks identical color wise and the graphic appears the same &lt;br &gt;&lt;br &gt;i am not entirely sure how to describe the taste other than to compare it to other curries this particular variety lacks the bite of the other nguan soon curries or other branded thai curry pastes that you may be familiar with from shopping in your local asian market more than likely you can buy this there as well it is a mellower darker colored brownish curry this is not your average madras yellow curry not that there is anything wrong with madras curry i keep both of these on my shelves and enjoy several other varieties &lt;br &gt;&lt;br &gt;my wife complains about the high turmeric smell of this one where she is not bothered by the madras variety overly much however the distinctive brown color this has over yellow leads me to wonder if the ratio of another ingredient is greater than the turmeric even though it is not as persistent a smell i use this in sandwiches quite often and just about anything else for its mellow contrast to the standard yellow curry i normally have access to locally i am probably the closest anyone in this state comes to be being lister from red dwarf while still being alive and lacking the curry stained clothing happy eating</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Summary Predicted Summary  \\\n",
       "0  used to love now disappointed   great coffee       \n",
       "1  good but still having gas       great product      \n",
       "2  cinnamon way too strong         great tea          \n",
       "3  flavorful seeds                 great pecans       \n",
       "4  a nice mellow wash of turmeric  great standby      \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Text  \n",
       "0  i used to love this decaf was my favorite great flavor non bitter but good flavor bordering on strong which i liked the last carton i received through amazon com is weak i can only use the small cup setting to make it okay i used to use the larger setting and add a little extra water has the amount of coffee in the k cups been reduced if the next batch is the same i will be looking for another brand                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "1  my twins took this formula since they were born sometimes they having gas with this kind of formula sensitive i think they are no miracles formula                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "2  i love peach tea and anything peach flavored but this has has a cinnamon flavor that is too strong i wanted so badly to give it 5 stars but i would not be honest if i did i do not like the cinnamon flavor to me peach and cinnamon do not belong together peach is summer and cinnamon is winter this tea might be better in the winter as hot tea but then again not because peach is too summery cinnamon would go better with apple flavor as soon as i finish this box i will not buy it again <br ><br >the cinnamon is way too strong it made me sick at my stomach and the aftertaste is horrible                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "3  i am absolutely thrilled to have 2 lbs of the frontier natural products whole cumin seed 16 ounce bags pack of 2 that i purchased directly from amazon they are delicious and flavorful since these are the whole seeds they should remain tasty for years i also have enough extra to make spice mixes for gifts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "4  firstly i may be reviewing a slightly different nguan soon curry product than what is being sold i normally buy this at a wonderful thai owned asian market in lexington my bottle from nguan soon is the same as the picture except the labeling languages are slightly different i believe there is vietnamese on mine instead as it says b t curry right under curry powder after googling the word and translating from vietnamese to french to english it appears to mean flour or powder so that is my cautionary notice should you buy this powder be aware that my bottle lists as its ingredients the following turmeric coriander cumin chili salt and pepper it does not mention garlic at all on my bottle but looks identical color wise and the graphic appears the same <br ><br >i am not entirely sure how to describe the taste other than to compare it to other curries this particular variety lacks the bite of the other nguan soon curries or other branded thai curry pastes that you may be familiar with from shopping in your local asian market more than likely you can buy this there as well it is a mellower darker colored brownish curry this is not your average madras yellow curry not that there is anything wrong with madras curry i keep both of these on my shelves and enjoy several other varieties <br ><br >my wife complains about the high turmeric smell of this one where she is not bothered by the madras variety overly much however the distinctive brown color this has over yellow leads me to wonder if the ratio of another ingredient is greater than the turmeric even though it is not as persistent a smell i use this in sandwiches quite often and just about anything else for its mellow contrast to the standard yellow curry i normally have access to locally i am probably the closest anyone in this state comes to be being lister from red dwarf while still being alive and lacking the curry stained clothing happy eating  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_df = pd.DataFrame({\n",
    "    'Average F1':avg_rouge_f1,\n",
    "    'Average Precision': avg_rouge_precision,\n",
    "    'Average Recalls': avg_rouge_recall}, index=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average F1</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recalls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Average F1  Average Precision  Average Recalls\n",
       " 0.06        0.10               0.05            "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "epochs = 5 ## We initially wanted to run for 10+ epochs, but couldn't afford to do to extremely long traning time\n",
    "batch_size = 32\n",
    "rnn_size = 128\n",
    "num_layers = 1\n",
    "learning_rate = 0.008\n",
    "keep_probability = 0.8\n",
    "direction = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters, wikihow\n",
    "epochs = 3 ## We initially wanted to run for 10+ epochs, but couldn't afford to do to extremely long traning time\n",
    "batch_size = 32\n",
    "rnn_size = 128\n",
    "num_layers = 1\n",
    "learning_rate = 0.008\n",
    "keep_probability = 0.8\n",
    "direction = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame({\n",
    "    'Amazon Review':[5, 32, 128, 1, 0.008, 0.8, 2],\n",
    "    'Wikihow': [3, 32, 128, 1, 0.008, 0.8, 2]}, \n",
    "    index = ['Epochs', 'Batch size', 'RNN Size', \n",
    "             'Num of Layers', 'Learning Rate', \n",
    "             'Keep Probability', 'Direction']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amazon Review</th>\n",
       "      <th>Wikihow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Epochs</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Batch size</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RNN Size</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num of Layers</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning Rate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keep Probability</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direction</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Amazon Review  Wikihow\n",
       "Epochs            5              3      \n",
       "Batch size        32             32     \n",
       "RNN Size          128            128    \n",
       "Num of Layers     1              1      \n",
       "Learning Rate     0              0      \n",
       "Keep Probability  0              0      \n",
       "Direction         2              2      "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other models from research papers, and their ROUGE metrics\n",
    "paper_ROUGE = pd.DataFrame({\n",
    "    'ROUGE-1':[39.2, 39.6, 35.46, 37.86, 38.30, 41.16, 39.87]}, \n",
    "    index = ['Lead-3 (Nallapati et al., 2017)', 'SummaRuNNer (Nallapati et al., 2017)',\n",
    "            'words-lvt2k-temp-att (Nallapati et al., 2016)', 'ML, no intra-attention (Paulus et al., 2017)',\n",
    "             'ML, with intra-attention (Paulus et al., 2017)', 'RL, with intra-attention (Paulus et al., 2017)', \n",
    "             'ML+RL, with intra-attention (Paulus et al., 2017)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROUGE-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lead-3 (Nallapati et al., 2017)</th>\n",
       "      <td>39.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SummaRuNNer (Nallapati et al., 2017)</th>\n",
       "      <td>39.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words-lvt2k-temp-att (Nallapati et al., 2016)</th>\n",
       "      <td>35.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML, no intra-attention (Paulus et al., 2017)</th>\n",
       "      <td>37.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML, with intra-attention (Paulus et al., 2017)</th>\n",
       "      <td>38.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL, with intra-attention (Paulus et al., 2017)</th>\n",
       "      <td>41.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML+RL, with intra-attention (Paulus et al., 2017)</th>\n",
       "      <td>39.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ROUGE-1\n",
       "Lead-3 (Nallapati et al., 2017)                   39.20   \n",
       "SummaRuNNer (Nallapati et al., 2017)              39.60   \n",
       "words-lvt2k-temp-att (Nallapati et al., 2016)     35.46   \n",
       "ML, no intra-attention (Paulus et al., 2017)      37.86   \n",
       "ML, with intra-attention (Paulus et al., 2017)    38.30   \n",
       "RL, with intra-attention (Paulus et al., 2017)    41.16   \n",
       "ML+RL, with intra-attention (Paulus et al., 2017) 39.87   "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
