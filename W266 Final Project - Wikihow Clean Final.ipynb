{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing: Loading in reviews data and process with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import pandas and set display option\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overview</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>sectionLabel</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So you're a new or aspiring artist and your c...</td>\n",
       "      <td>\\nSell yourself first.</td>\n",
       "      <td>Before doing anything else, stop and sum up y...</td>\n",
       "      <td>Steps</td>\n",
       "      <td>How to Sell Fine Art Online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you want to be well-read, then, in the wor...</td>\n",
       "      <td>\\nRead the classics before 1600.</td>\n",
       "      <td>Reading the classics is the very first thing ...</td>\n",
       "      <td>Reading the Classics</td>\n",
       "      <td>How to Be Well Read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So you're a new or aspiring artist and your c...</td>\n",
       "      <td>\\nJoin online artist communities.</td>\n",
       "      <td>Depending on what scale you intend to sell yo...</td>\n",
       "      <td>Steps</td>\n",
       "      <td>How to Sell Fine Art Online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So you're a new or aspiring artist and your c...</td>\n",
       "      <td>\\nMake yourself public.</td>\n",
       "      <td>Get yourself out there as best as you can by ...</td>\n",
       "      <td>Steps</td>\n",
       "      <td>How to Sell Fine Art Online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So you're a new or aspiring artist and your c...</td>\n",
       "      <td>\\nBlog about your artwork.</td>\n",
       "      <td>Given the hundreds of free blogging websites,...</td>\n",
       "      <td>Steps</td>\n",
       "      <td>How to Sell Fine Art Online</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            overview  \\\n",
       "0   So you're a new or aspiring artist and your c...   \n",
       "1   If you want to be well-read, then, in the wor...   \n",
       "2   So you're a new or aspiring artist and your c...   \n",
       "3   So you're a new or aspiring artist and your c...   \n",
       "4   So you're a new or aspiring artist and your c...   \n",
       "\n",
       "                            headline  \\\n",
       "0             \\nSell yourself first.   \n",
       "1   \\nRead the classics before 1600.   \n",
       "2  \\nJoin online artist communities.   \n",
       "3            \\nMake yourself public.   \n",
       "4         \\nBlog about your artwork.   \n",
       "\n",
       "                                                text          sectionLabel  \\\n",
       "0   Before doing anything else, stop and sum up y...                 Steps   \n",
       "1   Reading the classics is the very first thing ...  Reading the Classics   \n",
       "2   Depending on what scale you intend to sell yo...                 Steps   \n",
       "3   Get yourself out there as best as you can by ...                 Steps   \n",
       "4   Given the hundreds of free blogging websites,...                 Steps   \n",
       "\n",
       "                         title  \n",
       "0  How to Sell Fine Art Online  \n",
       "1          How to Be Well Read  \n",
       "2  How to Sell Fine Art Online  \n",
       "3  How to Sell Fine Art Online  \n",
       "4  How to Sell Fine Art Online  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Load wikihow dataset\n",
    "df2 = pd.read_csv(\"wikihowSep.csv\")\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summary_wikihow = df2.filter(items =['headline','text'])\n",
    "text_summary_wikihow = text_summary_wikihow.dropna()\n",
    "text_summary_wikihow = text_summary_wikihow.drop_duplicates()\n",
    "text_summary_wikihow = text_summary_wikihow.rename(columns = {'headline':'Summary', 'text':'Text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nSell yourself first.</td>\n",
       "      <td>Before doing anything else, stop and sum up y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nRead the classics before 1600.</td>\n",
       "      <td>Reading the classics is the very first thing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nJoin online artist communities.</td>\n",
       "      <td>Depending on what scale you intend to sell yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nMake yourself public.</td>\n",
       "      <td>Get yourself out there as best as you can by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nBlog about your artwork.</td>\n",
       "      <td>Given the hundreds of free blogging websites,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Summary  \\\n",
       "0             \\nSell yourself first.   \n",
       "1   \\nRead the classics before 1600.   \n",
       "2  \\nJoin online artist communities.   \n",
       "3            \\nMake yourself public.   \n",
       "4         \\nBlog about your artwork.   \n",
       "\n",
       "                                                Text  \n",
       "0   Before doing anything else, stop and sum up y...  \n",
       "1   Reading the classics is the very first thing ...  \n",
       "2   Depending on what scale you intend to sell yo...  \n",
       "3   Get yourself out there as best as you can by ...  \n",
       "4   Given the hundreds of free blogging websites,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get an idea of the summaries and text columns \n",
    "text_summary_wikihow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data cleaning to remove unwanted symbols\n",
    "import re\n",
    "def clean_text(sentences):\n",
    "    clean=[]\n",
    "    \n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''    \n",
    "    for sentence in sentences:\n",
    "        for key in contractions.keys():\n",
    "            sentence = sentence.lower().replace(key, contractions[key])\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "        sentence = re.sub(r'\\<a href', ' ', sentence)\n",
    "        sentence = re.sub(r'&amp;', '', sentence) \n",
    "        sentence = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', sentence)\n",
    "        sentence = re.sub(r'<br />', ' ', sentence)\n",
    "        sentence = re.sub(r'\\'', ' ', sentence)\n",
    "        \n",
    "        words = [word for word in sentence.split()]\n",
    "        clean.append(words)\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts_wikihow = clean_text(text_summary_wikihow['Text'])\n",
    "Summary_wikihow = clean_text(text_summary_wikihow['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1375354"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the size of the cleaned data\n",
    "len(Summary_wikihow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reading',\n",
       " 'the',\n",
       " 'classics',\n",
       " 'is',\n",
       " 'the',\n",
       " 'very',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'you',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'to',\n",
       " 'be',\n",
       " 'well',\n",
       " 'read',\n",
       " 'if',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'build',\n",
       " 'a',\n",
       " 'solid',\n",
       " 'foundation',\n",
       " 'for',\n",
       " 'your',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'the',\n",
       " 'books',\n",
       " 'you',\n",
       " 'read',\n",
       " 'then',\n",
       " 'you',\n",
       " 'cannot',\n",
       " 'avoid',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earliest',\n",
       " 'plays',\n",
       " 'poems',\n",
       " 'and',\n",
       " 'oral',\n",
       " 'tales',\n",
       " 'ever',\n",
       " 'written',\n",
       " 'down',\n",
       " 'remember',\n",
       " 'that',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'did',\n",
       " 'not',\n",
       " 'really',\n",
       " 'get',\n",
       " 'popular',\n",
       " 'until',\n",
       " 'the',\n",
       " '18th',\n",
       " 'century',\n",
       " 'so',\n",
       " 'you',\n",
       " 'will',\n",
       " 'not',\n",
       " 'find',\n",
       " 'novels',\n",
       " 'on',\n",
       " 'this',\n",
       " 'list',\n",
       " 'without',\n",
       " 'reading',\n",
       " 'the',\n",
       " 'poetry',\n",
       " 'of',\n",
       " 'homer',\n",
       " 'or',\n",
       " 'the',\n",
       " 'plays',\n",
       " 'of',\n",
       " 'sophocles',\n",
       " 'you',\n",
       " 'will',\n",
       " 'not',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'call',\n",
       " 'yourself',\n",
       " 'well',\n",
       " 'read',\n",
       " 'here',\n",
       " 's',\n",
       " 'a',\n",
       " 'list',\n",
       " 'to',\n",
       " 'get',\n",
       " 'you',\n",
       " 'started',\n",
       " 'the',\n",
       " 'epic',\n",
       " 'of',\n",
       " 'gilgamesh',\n",
       " 'unknown',\n",
       " 'author',\n",
       " '18th',\n",
       " '–',\n",
       " '17th',\n",
       " 'century',\n",
       " 'bce',\n",
       " 'the',\n",
       " 'iliad',\n",
       " 'and',\n",
       " 'the',\n",
       " 'odyssey',\n",
       " 'by',\n",
       " 'homer',\n",
       " '850–750',\n",
       " 'bce',\n",
       " '8th',\n",
       " 'century',\n",
       " 'bce',\n",
       " 'the',\n",
       " 'oresteia',\n",
       " 'by',\n",
       " 'aeschylus',\n",
       " '458',\n",
       " 'bce',\n",
       " 'oedipus',\n",
       " 'the',\n",
       " 'king',\n",
       " 'by',\n",
       " 'sophocles',\n",
       " '430',\n",
       " 'bce',\n",
       " 'medea',\n",
       " 'by',\n",
       " 'euripides',\n",
       " '431',\n",
       " 'bce',\n",
       " 'aeneid',\n",
       " 'by',\n",
       " 'virgil',\n",
       " '29–19',\n",
       " 'bce',\n",
       " 'one',\n",
       " 'thousand',\n",
       " 'and',\n",
       " 'one',\n",
       " 'nights',\n",
       " 'unknown',\n",
       " 'author',\n",
       " '700–1500',\n",
       " 'beowulf',\n",
       " 'unknown',\n",
       " 'author',\n",
       " '975',\n",
       " '1025',\n",
       " 'the',\n",
       " 'tale',\n",
       " 'of',\n",
       " 'genji',\n",
       " 'by',\n",
       " 'murasaki',\n",
       " 'shikibu',\n",
       " '11th',\n",
       " 'century',\n",
       " 'the',\n",
       " 'divine',\n",
       " 'comedy',\n",
       " 'by',\n",
       " 'dante',\n",
       " '1265–1321',\n",
       " 'the',\n",
       " 'decameron',\n",
       " 'by',\n",
       " 'boccaccio',\n",
       " '1349–53',\n",
       " 'the',\n",
       " 'canterbury',\n",
       " 'tales',\n",
       " 'by',\n",
       " 'chaucer',\n",
       " '14th',\n",
       " 'century']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verify that one entry of the clearned data\n",
    "Texts_wikihow[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read', 'the', 'classics', 'before', '1600']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summary_wikihow[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to count word frequency\n",
    "def count_words(count_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence:\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 268289\n"
     ]
    }
   ],
   "source": [
    "## Apply functino above to generate the word frequency for each word and get vocab size in dataset\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, Summary_wikihow)\n",
    "count_words(word_counts, Texts_wikihow)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understand how long summaries and texts generally are, so that we can exclude outliers later\n",
    "def create_lengths(text):\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "          counts\n",
      "count 1375354.00\n",
      "mean        7.44\n",
      "std         6.17\n",
      "min         0.00\n",
      "25%         4.00\n",
      "50%         6.00\n",
      "75%         9.00\n",
      "max      2952.00\n",
      "\n",
      "Texts:\n",
      "          counts\n",
      "count 1375354.00\n",
      "mean       68.99\n",
      "std        54.85\n",
      "min         0.00\n",
      "25%        30.00\n",
      "50%        57.00\n",
      "75%        94.00\n",
      "max      2849.00\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(Summary_wikihow)\n",
    "lengths_texts = create_lengths(Texts_wikihow)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Glove\n",
    "#Can select from 50, 100, 200 and 300 for dimensions\n",
    "import numpy as np\n",
    "def parse_glove(dimension):\n",
    "    filename = \"glove.6B.{:d}d.txt\".format(dimension)\n",
    "    embeddings_index={}\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row=line.strip().split(' ')\n",
    "        word = row[0]\n",
    "        embedding = np.asarray(row[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "    print('Word embeddings:', len(embeddings_index))\n",
    "    file.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 400000\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 200 #set dimension to 200\n",
    "embeddings_index = parse_glove(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from GloVe: 8782\n",
      "Percent of words that are missing: 3.27%\n"
     ]
    }
   ],
   "source": [
    "## Find the number of words that are missing from our data from GloVe, and appears more often than our set threshold\n",
    "missing_words = 0\n",
    "threshold = 10\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from GloVe:\", missing_words)\n",
    "print(\"Percent of words that are missing: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 268289\n",
      "Number of words we will use: 112898\n"
     ]
    }
   ],
   "source": [
    "## create mapping to map each word to an integer ID\n",
    "word_to_id = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        word_to_id[word] = value\n",
    "        value += 1\n",
    "\n",
    "## Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "## Add tokens to vocab\n",
    "for code in codes:\n",
    "    word_to_id[code] = len(word_to_id)\n",
    "\n",
    "## Create mappingn for ID to word\n",
    "id_to_word = {}\n",
    "for word, value in word_to_id.items():\n",
    "    id_to_word[value] = word\n",
    "\n",
    "\n",
    "print(\"Number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to find nearest neighbor for unknown words\n",
    "W = list(embeddings_index.values())\n",
    "vocab = list(embeddings_index.keys())\n",
    "def nearest_neighbor(word):\n",
    "    v= embeddings_index[word]\n",
    "    dot_product = np.dot(W,v)\n",
    "    A = np.sqrt(np.sum(np.square(v),0))\n",
    "    B = np.sqrt(np.sum(np.square(W),1))\n",
    "    denominator = np.multiply(A,B)\n",
    "    cosine_similarities = np.divide(dot_product,denominator)\n",
    "    v_sim = W[np.argmax(cosine_similarities)]\n",
    "    for x in range(0,len(W)):\n",
    "        if np.array_equal(W[x],np.asarray(v_sim)):\n",
    "            return vocab[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Createa a reduced embedding dictionary\n",
    "word_embedding_matrix = np.zeros((len(word_to_id), embedding_dim), dtype=np.float32)\n",
    "for word, i in word_to_id.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in GloVe, we start with a randomized embedding\n",
    "        rand_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = rand_embedding\n",
    "        word_embedding_matrix[i] = rand_embedding\n",
    "## We also attempted to use the nearest neighbor for unknown words (function above)\n",
    "## We found this reduces speed rather significantly, and given the low % of NNs, we proceeded with the randomized embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert words to IDs (we do not convert directly to embeddings to increase the performance)\n",
    "def convert_to_ids(text, word_n, unk_n, eos=False): \n",
    "    ids = []\n",
    "    for sentence in text:\n",
    "        sentence_ids = []\n",
    "        for word in sentence:\n",
    "            word_n += 1\n",
    "            if word in word_to_id:\n",
    "                sentence_ids.append(word_to_id[word])\n",
    "            else:\n",
    "                sentence_ids.append(word_to_id[\"<UNK>\"])\n",
    "                unk_n += 1\n",
    "        if eos:\n",
    "            sentence_ids.append(word_to_id[\"<EOS>\"])\n",
    "        ids.append(sentence_ids)\n",
    "    return ids, word_n, unk_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of UNK: 0.27%\n"
     ]
    }
   ],
   "source": [
    "## Apply the convert_to_ids function\n",
    "word_n = 0\n",
    "unk_n = 0\n",
    "\n",
    "id_summaries, word_n, unk_n_sum = convert_to_ids(Summary_wikihow, word_n, unk_n)\n",
    "id_texts, word_n, unk_n_text = convert_to_ids(Texts_wikihow, word_n, unk_n, eos=True)\n",
    "unk_percent = round((unk_n_sum+unk_n_text)/word_n,4)*100\n",
    "\n",
    "print(\"Percent of UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unk(text):\n",
    "    unk_n = text.count(word_to_id[\"<UNK>\"])\n",
    "    return unk_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963337\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences by length to reduce padding\n",
    "sorted_id_summaries = []\n",
    "sorted_id_texts = []\n",
    "max_text_length = 100 #75% percentile\n",
    "min_summary_length = 2 #want to produce summaries more than 2 words\n",
    "min_text_length = 10 #ensure meaningful text worth summarizing\n",
    "max_unk_text = 2\n",
    "max_unk_summary = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for i, words in enumerate(id_summaries):\n",
    "        if (len(id_summaries[i]) >= min_summary_length and\n",
    "            len(id_texts[i])>= min_text_length and\n",
    "            len(id_texts[i])<= max_text_length and\n",
    "            count_unk(id_summaries[i])<=max_unk_summary and\n",
    "            count_unk(id_texts[i])<=max_unk_text and\n",
    "            len(id_texts[i]) == length):\n",
    "            \n",
    "            sorted_id_summaries.append(id_summaries[i])\n",
    "            sorted_id_texts.append(id_texts[i])\n",
    "            \n",
    "print(len(sorted_id_summaries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the convered IDs for texts and summaries so we don't have to rerun data processing again\n",
    "np.save(\"wikihow_id_summaries\",sorted_id_summaries)\n",
    "np.save(\"wikihow_id_texts\",sorted_id_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_summaries = np.load('wikihow_id_summaries.npy')\n",
    "# id_texts = np.load('wikihow_id_texts.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade tensorflow\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to create tf.placeholders for hyper parameters and model inputs\n",
    "def model_inputs():\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to process input for decoding input (adding <GO> to begining of batch)\n",
    "def process_decoding_input(target_data, word_to_id, batch_size):\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], word_to_id['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create encoding layer function - can choose between bi-direction or one direction LSTM\n",
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
    "    \n",
    "    if direction == 1:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "\n",
    "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                         input_keep_prob = keep_prob)\n",
    "\n",
    "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n",
    "                                                              rnn_inputs,\n",
    "                                                              sequence_length,\n",
    "                                                              dtype=tf.float32)\n",
    "\n",
    "            return enc_output, enc_state\n",
    "        \n",
    "        \n",
    "    if direction == 2:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                            input_keep_prob = keep_prob)\n",
    "\n",
    "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                            input_keep_prob = keep_prob)\n",
    "\n",
    "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                            cell_bw, \n",
    "                                                                            rnn_inputs,\n",
    "                                                                            sequence_length,\n",
    "                                                                            dtype=tf.float32)\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            return enc_output, enc_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training logits\n",
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)    \n",
    "    \n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ ,_  = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoding layer with attention (Bahdanau) for training and inference\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, word_to_id, keep_prob, batch_size, num_layers, direction):\n",
    "    \n",
    "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
    "        for layer in range(num_layers):\n",
    "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "                lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    with tf.name_scope(\"Attention_Wrapper\"):\n",
    "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
    "            \n",
    "        \n",
    "    initial_state = dec_cell.zero_state(batch_size,dtype=tf.float32)\n",
    "    initial_state = initial_state.clone(cell_state = enc_state)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,  \n",
    "                                                    word_to_id['<GO>'], \n",
    "                                                    word_to_id['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, word_to_id, batch_size, direction):\n",
    "    \n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob, direction)\n",
    "    \n",
    "    dec_input = process_decoding_input(target_data, word_to_id, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        word_to_id, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers, direction)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create padding so each batch has the same sentence length\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [word_to_id['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "epochs = 3 ## We initially wanted to run for 10+ epochs, but couldn't afford to do to extremely long traning time\n",
    "batch_size = 32\n",
    "rnn_size = 128\n",
    "num_layers = 1\n",
    "learning_rate = 0.008\n",
    "keep_probability = 0.8\n",
    "direction = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/davidhe/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /Users/davidhe/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(word_to_id)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      word_to_id,\n",
    "                                                      batch_size, \n",
    "                                                      direction)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"cost\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits,targets,masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a training subset\n",
    "start = 0\n",
    "end = start + 10000\n",
    "Summaries_short = id_summaries[start:end]\n",
    "Texts_short = id_texts[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/3 Batch   10/312 - Loss:  5.821, Seconds: 571.02\n",
      "Epoch   1/3 Batch   20/312 - Loss:  3.311, Seconds: 607.27\n",
      "Epoch   1/3 Batch   30/312 - Loss:  2.466, Seconds: 1186.20\n",
      "Epoch   1/3 Batch   40/312 - Loss:  3.103, Seconds: 468.10\n",
      "Epoch   1/3 Batch   50/312 - Loss:  2.885, Seconds: 411.86\n",
      "Epoch   1/3 Batch   60/312 - Loss:  3.160, Seconds: 1419.31\n",
      "Epoch   1/3 Batch   70/312 - Loss:  3.190, Seconds: 870.37\n",
      "Epoch   1/3 Batch   80/312 - Loss:  2.618, Seconds: 653.26\n",
      "Epoch   1/3 Batch   90/312 - Loss:  2.357, Seconds: 543.66\n",
      "Epoch   1/3 Batch  100/312 - Loss:  2.391, Seconds: 489.86\n",
      "Average loss for this update: 3.104\n",
      "New Record!\n",
      "Epoch   1/3 Batch  110/312 - Loss:  2.461, Seconds: 359.58\n",
      "Epoch   1/3 Batch  120/312 - Loss:  2.027, Seconds: 277.04\n",
      "Epoch   1/3 Batch  130/312 - Loss:  1.968, Seconds: 204.20\n",
      "Epoch   1/3 Batch  140/312 - Loss:  2.604, Seconds: 324.39\n",
      "Epoch   1/3 Batch  150/312 - Loss:  2.453, Seconds: 289.57\n",
      "Epoch   1/3 Batch  160/312 - Loss:  2.638, Seconds: 323.88\n",
      "Epoch   1/3 Batch  170/312 - Loss:  2.705, Seconds: 397.58\n",
      "Epoch   1/3 Batch  180/312 - Loss:  2.460, Seconds: 382.07\n",
      "Epoch   1/3 Batch  190/312 - Loss:  2.642, Seconds: 376.83\n",
      "Epoch   1/3 Batch  200/312 - Loss:  3.143, Seconds: 327.05\n",
      "Average loss for this update: 2.509\n",
      "New Record!\n",
      "Epoch   1/3 Batch  210/312 - Loss:  2.276, Seconds: 518.57\n",
      "Epoch   1/3 Batch  220/312 - Loss:  1.956, Seconds: 394.83\n",
      "Epoch   1/3 Batch  230/312 - Loss:  1.700, Seconds: 553.85\n",
      "Epoch   1/3 Batch  240/312 - Loss:  1.883, Seconds: 377.73\n",
      "Epoch   1/3 Batch  250/312 - Loss:  2.738, Seconds: 448.99\n",
      "Epoch   1/3 Batch  260/312 - Loss:  2.336, Seconds: 306.45\n",
      "Epoch   1/3 Batch  270/312 - Loss:  2.366, Seconds: 327.23\n",
      "Epoch   1/3 Batch  280/312 - Loss:  2.139, Seconds: 395.56\n",
      "Epoch   1/3 Batch  290/312 - Loss:  2.269, Seconds: 972.12\n",
      "Epoch   1/3 Batch  300/312 - Loss:  2.345, Seconds: 1401.08\n",
      "Average loss for this update: 2.233\n",
      "New Record!\n",
      "Epoch   1/3 Batch  310/312 - Loss:  2.629, Seconds: 854.38\n",
      "Epoch   2/3 Batch   10/312 - Loss:  2.632, Seconds: 928.13\n",
      "Epoch   2/3 Batch   20/312 - Loss:  2.538, Seconds: 928.75\n",
      "Epoch   2/3 Batch   30/312 - Loss:  1.927, Seconds: 1707.05\n",
      "Epoch   2/3 Batch   40/312 - Loss:  2.512, Seconds: 684.76\n",
      "Epoch   2/3 Batch   50/312 - Loss:  2.312, Seconds: 605.51\n",
      "Epoch   2/3 Batch   60/312 - Loss:  2.625, Seconds: 2000.22\n",
      "Epoch   2/3 Batch   70/312 - Loss:  2.657, Seconds: 1297.12\n",
      "Epoch   2/3 Batch   80/312 - Loss:  2.176, Seconds: 956.30\n",
      "Epoch   2/3 Batch   90/312 - Loss:  1.989, Seconds: 652.80\n",
      "Epoch   2/3 Batch  100/312 - Loss:  1.983, Seconds: 542.48\n",
      "Average loss for this update: 2.323\n",
      "No Improvement.\n",
      "Epoch   2/3 Batch  110/312 - Loss:  2.045, Seconds: 539.90\n",
      "Epoch   2/3 Batch  120/312 - Loss:  1.611, Seconds: 408.65\n",
      "Epoch   2/3 Batch  130/312 - Loss:  1.647, Seconds: 299.85\n",
      "Epoch   2/3 Batch  140/312 - Loss:  2.240, Seconds: 485.24\n",
      "Epoch   2/3 Batch  150/312 - Loss:  2.143, Seconds: 431.15\n",
      "Epoch   2/3 Batch  160/312 - Loss:  2.134, Seconds: 481.76\n",
      "Epoch   2/3 Batch  170/312 - Loss:  2.169, Seconds: 587.08\n",
      "Epoch   2/3 Batch  180/312 - Loss:  2.073, Seconds: 558.98\n",
      "Epoch   2/3 Batch  190/312 - Loss:  2.282, Seconds: 562.02\n",
      "Epoch   2/3 Batch  200/312 - Loss:  2.797, Seconds: 483.23\n",
      "Average loss for this update: 2.12\n",
      "New Record!\n",
      "Epoch   2/3 Batch  210/312 - Loss:  2.043, Seconds: 767.76\n",
      "Epoch   2/3 Batch  220/312 - Loss:  1.728, Seconds: 590.80\n",
      "Epoch   2/3 Batch  230/312 - Loss:  1.524, Seconds: 817.11\n",
      "Epoch   2/3 Batch  240/312 - Loss:  1.702, Seconds: 558.24\n",
      "Epoch   2/3 Batch  250/312 - Loss:  2.363, Seconds: 447.42\n",
      "Epoch   2/3 Batch  260/312 - Loss:  2.083, Seconds: 319.28\n",
      "Epoch   2/3 Batch  270/312 - Loss:  2.101, Seconds: 325.78\n",
      "Epoch   2/3 Batch  280/312 - Loss:  1.919, Seconds: 394.76\n",
      "Epoch   2/3 Batch  290/312 - Loss:  2.048, Seconds: 750.87\n",
      "Epoch   2/3 Batch  300/312 - Loss:  2.122, Seconds: 1075.01\n",
      "Average loss for this update: 1.993\n",
      "New Record!\n",
      "Epoch   2/3 Batch  310/312 - Loss:  2.362, Seconds: 761.56\n",
      "Epoch   3/3 Batch   10/312 - Loss:  2.484, Seconds: 570.04\n",
      "Epoch   3/3 Batch   20/312 - Loss:  2.426, Seconds: 609.92\n",
      "Epoch   3/3 Batch   30/312 - Loss:  1.822, Seconds: 1170.18\n",
      "Epoch   3/3 Batch   40/312 - Loss:  2.372, Seconds: 464.63\n",
      "Epoch   3/3 Batch   50/312 - Loss:  2.172, Seconds: 409.75\n",
      "Epoch   3/3 Batch   60/312 - Loss:  2.461, Seconds: 1366.51\n",
      "Epoch   3/3 Batch   70/312 - Loss:  2.489, Seconds: 853.14\n",
      "Epoch   3/3 Batch   80/312 - Loss:  2.036, Seconds: 822.98\n",
      "Epoch   3/3 Batch   90/312 - Loss:  1.907, Seconds: 902.18\n",
      "Epoch   3/3 Batch  100/312 - Loss:  1.842, Seconds: 377.60\n",
      "Average loss for this update: 2.189\n",
      "No Improvement.\n",
      "Epoch   3/3 Batch  110/312 - Loss:  1.921, Seconds: 372.65\n",
      "Epoch   3/3 Batch  120/312 - Loss:  1.493, Seconds: 281.31\n",
      "Epoch   3/3 Batch  130/312 - Loss:  1.538, Seconds: 208.44\n",
      "Epoch   3/3 Batch  140/312 - Loss:  2.082, Seconds: 337.09\n",
      "Epoch   3/3 Batch  150/312 - Loss:  2.043, Seconds: 299.68\n",
      "Epoch   3/3 Batch  160/312 - Loss:  1.949, Seconds: 337.47\n",
      "Epoch   3/3 Batch  170/312 - Loss:  1.998, Seconds: 394.24\n",
      "Epoch   3/3 Batch  180/312 - Loss:  1.964, Seconds: 378.92\n",
      "Epoch   3/3 Batch  190/312 - Loss:  2.144, Seconds: 376.40\n",
      "Epoch   3/3 Batch  200/312 - Loss:  2.582, Seconds: 324.62\n",
      "Average loss for this update: 1.976\n",
      "New Record!\n",
      "Epoch   3/3 Batch  210/312 - Loss:  1.900, Seconds: 518.64\n",
      "Epoch   3/3 Batch  220/312 - Loss:  1.621, Seconds: 393.93\n",
      "Epoch   3/3 Batch  230/312 - Loss:  1.435, Seconds: 552.56\n",
      "Epoch   3/3 Batch  240/312 - Loss:  1.606, Seconds: 376.39\n",
      "Epoch   3/3 Batch  250/312 - Loss:  2.216, Seconds: 449.88\n",
      "Epoch   3/3 Batch  260/312 - Loss:  1.939, Seconds: 306.14\n",
      "Epoch   3/3 Batch  270/312 - Loss:  1.957, Seconds: 358.91\n",
      "Epoch   3/3 Batch  280/312 - Loss:  1.799, Seconds: 394.39\n",
      "Epoch   3/3 Batch  290/312 - Loss:  1.945, Seconds: 751.87\n",
      "Epoch   3/3 Batch  300/312 - Loss:  1.987, Seconds: 1069.58\n",
      "Average loss for this update: 1.87\n",
      "New Record!\n",
      "Epoch   3/3 Batch  310/312 - Loss:  2.227, Seconds: 651.99\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 10 # Check training loss \n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(Texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(Summaries_short, Texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(Texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('Improved Result') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    save_path = saver.save(sess, \"./Checkpoint/model_wiki.ckpt\")\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Checkpoint/model_wiki.ckpt\n",
      "Original Text: ['you', 'are', 'still', 'best', 'friends', 'technically', 'so', 'do', 'not', 'let', 'old', 'habits', 'change', 'just', 'upgrade', 'them', 'when', 'you', 'have', 'that', 'date', 'officially', 'just', 'be', 'yourself', 'they', 'fell', 'in', 'love', 'with', 'their', 'best', 'friend', 'not', 'a', 'stranger', 'please', 'do', 'keep', 'that', 'in', 'mind', 'if', 'you', 'have', 'broken', 'the', 'touch', 'barrier', 'by', 'this', 'point', 'feel', 'free', 'to', 'show', 'some', 'affection', 'of', 'course', 'this', 'all', 'depends', 'on', 'you', 'and', 'your', 'more', 'than', 'friend', 's', 'level', 'of', 'comfort', 'with', 'one', 'another', 'and', 'physical', 'contact', 'itself']\n",
      "  Input Words: you are still best friends technically so do not let old habits change just upgrade them when you have that date officially just be yourself they fell in love with their best friend not a stranger please do keep that in mind if you have broken the touch barrier by this point feel free to show some affection of course this all depends on you and your more than friend s level of comfort with one another and physical contact itself\n",
      "\\Generated Summary\n",
      "  Response Words: make sure the shares of\n"
     ]
    }
   ],
   "source": [
    "## Pull a random sentence from the unseen text in data and see the prediction\n",
    "random = np.random.randint(end,len(Texts_wikihow))\n",
    "input_sentence = Texts_wikihow[random]\n",
    "text = [word_to_id.get(word, word_to_id['<UNK>']) for word in input_sentence]\n",
    "\n",
    "checkpoint = \"./Checkpoint/model_wiki.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from output\n",
    "pad = word_to_id[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "print('  Input Words: {}'.format(\" \".join([id_to_word[i] for i in text])))\n",
    "\n",
    "print('\\Generated Summary')\n",
    "print('  Response Words: {}'.format(\" \".join([id_to_word[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /Users/davidhe/anaconda/lib/python3.6/site-packages (0.3.1)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[31mtimestring 1.6.2 has requirement pytz==2013b, but you'll have pytz 2018.4 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install the library\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start out slow\n",
      "make sure the shares of\n"
     ]
    }
   ],
   "source": [
    "summary_sentence = Summary_wikihow[random]\n",
    "summary_text = [word_to_id.get(word, word_to_id['<UNK>']) for word in summary_sentence]\n",
    "original_summary = \" \".join([id_to_word[i] for i in summary_text])\n",
    "generated_summary = \" \".join([id_to_word[i] for i in answer_logits if i != pad])\n",
    "print(original_summary)\n",
    "print(generated_summary)\n",
    "# original_summary = \"this tea is my new alternative after jasmine green tea it is so smooth and light ican drink it anytime\"\n",
    "# generated_summary = \"great tea\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge_calculator = Rouge()\n",
    "scores = rouge_calculator.get_scores(original_summary, generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metric Name: rouge-1\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-score: 0.0\n",
      "\n",
      "Metric Name: rouge-2\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-score: 0.0\n",
      "\n",
      "Metric Name: rouge-l\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Pretty-print the results\n",
    "#note that this is just for ONE generated summary against corresponding summary from dataset \n",
    "for metric in scores:\n",
    "    for metric_name, metric_vals in metric.items():\n",
    "        print(\"\\nMetric Name: {}\\nPrecision: {}\\nRecall: {}\\nF-score: {}\".format(\n",
    "                metric_name, metric_vals['p'], metric_vals['r'], metric_vals['f']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE Metrics (Aggregate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select test texts and infer predicted summaries\n",
    "size = 100\n",
    "test_list = np.random.randint(10000,len(Texts_wikihow),size = size)\n",
    "summary_tests = [Summary_wikihow[i] for i in test_list]\n",
    "text_tests = [Texts_wikihow[i] for i in test_list]\n",
    "Predicted_Summaries = []\n",
    "checkpoint = \"./Checkpoint/model_wiki.ckpt\"\n",
    "pad = word_to_id[\"<PAD>\"] \n",
    "\n",
    "text_id_tests = []\n",
    "for text in text_tests:\n",
    "    ids = [word_to_id.get(word, word_to_id['<UNK>']) for word in text]\n",
    "    text_id_tests.append(ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_id_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Checkpoint/model_wiki.ckpt\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    for text in text_id_tests:\n",
    "        if len(text) == 0:\n",
    "            Predicted_Summaries.append('')            \n",
    "        else:\n",
    "            answer = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                          summary_length: [np.random.randint(5,8)], \n",
    "                                          text_length: [len(text)]*batch_size,\n",
    "                                          keep_prob: 1.0})[0] \n",
    "            pred = [id_to_word[i] for i in answer if i != pad]\n",
    "            Predicted_Summaries.append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_n = 1\n",
    "ngram_o = []\n",
    "for sent in summary_tests:\n",
    "    ngram_set_o = set()\n",
    "    text_length = len(sent)\n",
    "    max_index_ngram_start = text_length - rouge_n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set_o.add(tuple(sent[i:i + rouge_n]))\n",
    "    ngram_o.append(ngram_set_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_p = []\n",
    "for sent in Predicted_Summaries:\n",
    "    ngram_set_p = set()\n",
    "    text_length = len(sent)\n",
    "    max_index_ngram_start = text_length - rouge_n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set_p.add(tuple(sent[i:i + rouge_n]))\n",
    "    ngram_p.append(ngram_set_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for i in range(len(ngram_o)):\n",
    "    overlap = ngram_o[i].intersection(ngram_p[i])\n",
    "    overlap_n = len(overlap)\n",
    "    o_n = len(ngram_o[i])\n",
    "    p_n = len(ngram_p[i])\n",
    "    \n",
    "    if p_n == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlap_n / p_n\n",
    "\n",
    "    if o_n == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlap_n / o_n\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    \n",
    "    f1_scores.append(f1_score)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rouge_f1 = np.mean(f1_scores)\n",
    "avg_rouge_precision = np.mean(precisions)\n",
    "avg_rouge_recall = np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['use', 'action', 'verbs'], ['put', 'on', 'some', 'suspenders'], ['make', 'some', 'assumptions'], ['write', 'a', 'brief', 'introduction'], ['join', 'a', 'support', 'group']]\n"
     ]
    }
   ],
   "source": [
    "print(summary_tests[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['edit', 'your', 'sentences', 'so', 'that', 'your', 'verbs', 'are', 'consistent', 'and', 'add', 'color', 'to', 'your', 'work', 'experience', 'all', 'verbs', 'should', 'be', 'in', 'the', 'same', 'tenses', 'write', 'about', 'previous', 'projects', 'in', 'the', 'past', 'tense', 'but', 'the', 'description', 'of', 'a', 'job', 'function', 'or', 'institution', 'in', 'the', 'present', 'tense', 'all', 'verbs', 'should', 'be', 'in', 'the', 'active', 'not', 'passive', 'voice', 'use', 'verbs', 'that', 'match', 'or', 'are', 'synonymous', 'with', 'some', 'verbs', 'on', 'the', 'job', 'description', 'you', 'are', 'applying', 'for'], ['authentic', 'lederhosen', 'may', 'come', 'with', 'suspenders', 'but', 'if', 'you', 'buy', 'them', 'separately', 'try', 'to', 'find', 'some', 'that', 'match', 'the', 'color', 'of', 'your', 'breeches'], ['if', 'you', 'think', 'you', 'have', 'real', 'cause', 'to', 'suspect', 'your', 'spouse', 'then', 'start', 'with', 'the', 'assumption', 'that', 'she', 'is', 'going', 'to', 'take', 'some', 'kind', 'of', 'precautions', 'to', 'remain', 'undiscovered', 'when', 'cheating', 'she', 'is', 'not', 'going', 'to', 'send', 'emails', 'from', 'the', 'home', 'computer', 'or', 'call', 'from', 'the', 'home', 'phone', 'she', 'is', 'not', 'going', 'to', 'claim', 'to', 'be', 'working', 'late', 'and', 'leave', 'for', 'a', 'hotel', 'rendezvous', 'risking', 'your', 'calls', 'going', 'unanswered', 'or', 'being', 'seen', 'leaving', 'work', 'too', 'early', 'she', 'will', 'use', 'normal', 'routines', 'and', 'patterns', 'that', 'you', 'are', 'well', 'used', 'to', 'and', 'simply', 'use', 'that', 'time', 'to', 'have', 'the', 'affair', 'a', 'sexual', 'affair', 'does', 'not', 'require', 'much', 'time', 'or', 'commitment', 'the', 'two', 'of', 'them', 'meet', 'in', 'the', 'parking', 'lot', 'hop', 'into', 'one', 'car', 'head', 'for', 'their', 'room', 'at', 'the', 'motel', '9', 'for', 'a', 'half', 'hour', 'and', 'are', 'back', 'in', 'time', 'for', 'shopping', 'she', 'even', 'comes', 'home', 'with', 'purchases', 'consistent', 'with', 'where', 'they', 'were', 'supposed', 'to', 'be', 'so', 'if', 'you', 'are', 'truly', 'committed', 'to', 'finding', 'the', 'truth', 'do', 'this'], ['if', 'you', 'come', 'from', 'a', 'large', 'family', 'or', 'if', 'your', 'grandparent', 'had', 'a', 'lot', 'of', 'friends', 'there', 'is', 'a', 'chance', 'that', 'not', 'everyone', 'will', 'know', 'you', 'as', 'the', 'grandchild', 'keep', 'your', 'introduction', 'very', 'brief', 'just', 'a', 'short', 'sentence', 'will', 'suffice', 'the', 'introduction', 'should', 'simply', 'let', 'people', 'know', 'your', 'name', 'and', 'your', 'relation', 'to', 'the', 'deceased'], ['a', 'support', 'group', 'can', 'help', 'you', 'join', 'with', 'other', 'people', 'who', 'have', 'similar', 'obsessive', 'thoughts', 'or', 'fears', 'a', 'support', 'group', 'can', 'offer', 'encouragement', 'support', 'and', 'friendship', 'and', 'can', 'help', 'with', 'feelings', 'of', 'isolation', 'ask', 'your', 'medical', 'doctor', 'or', 'therapist', 'if', 'there', 'are', 'any', 'local', 'support', 'groups', 'that', 'deal', 'with', 'obsessive', 'thoughts']]\n"
     ]
    }
   ],
   "source": [
    "print(text_tests[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'to', 'the', 'doll'], ['make', 'sure', 'the', 'weather'], ['make', 'sure', 'your', 'hands'], ['make', 'sure', 'your', 'hands', 'and', 'the', 'morsel'], ['make', 'sure', 'the', 'shares', 'of', 'the', 'shares']]\n"
     ]
    }
   ],
   "source": [
    "print(Predicted_Summaries[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0946427100116\n",
      "0.129357142857\n",
      "0.0839063107945\n"
     ]
    }
   ],
   "source": [
    "#general average of selected number of generated summaries against corresponding summaries from data set\n",
    "print(avg_rouge_f1)\n",
    "print(avg_rouge_precision)\n",
    "print(avg_rouge_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "example_n = 5\n",
    "ex_summary = []\n",
    "ex_text = []\n",
    "pred_summary = []\n",
    "\n",
    "for i in summary_tests[:example_n]:\n",
    "    ex_summary.append(\" \".join(i))\n",
    "\n",
    "for j in text_tests[:example_n]:\n",
    "    ex_text.append(\" \".join(j))\n",
    "    \n",
    "for k in Predicted_Summaries[:example_n]:\n",
    "    pred_summary.append(\" \".join(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = pd.DataFrame({\n",
    "    'Summary': ex_summary,\n",
    "    'Predicted Summary': pred_summary,\n",
    "    'Text': ex_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Predicted Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use action verbs</td>\n",
       "      <td>go to the doll</td>\n",
       "      <td>edit your sentences so that your verbs are consistent and add color to your work experience all verbs should be in the same tenses write about previous projects in the past tense but the description of a job function or institution in the present tense all verbs should be in the active not passive voice use verbs that match or are synonymous with some verbs on the job description you are applying for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>put on some suspenders</td>\n",
       "      <td>make sure the weather</td>\n",
       "      <td>authentic lederhosen may come with suspenders but if you buy them separately try to find some that match the color of your breeches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>make some assumptions</td>\n",
       "      <td>make sure your hands</td>\n",
       "      <td>if you think you have real cause to suspect your spouse then start with the assumption that she is going to take some kind of precautions to remain undiscovered when cheating she is not going to send emails from the home computer or call from the home phone she is not going to claim to be working late and leave for a hotel rendezvous risking your calls going unanswered or being seen leaving work too early she will use normal routines and patterns that you are well used to and simply use that time to have the affair a sexual affair does not require much time or commitment the two of them meet in the parking lot hop into one car head for their room at the motel 9 for a half hour and are back in time for shopping she even comes home with purchases consistent with where they were supposed to be so if you are truly committed to finding the truth do this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>write a brief introduction</td>\n",
       "      <td>make sure your hands and the morsel</td>\n",
       "      <td>if you come from a large family or if your grandparent had a lot of friends there is a chance that not everyone will know you as the grandchild keep your introduction very brief just a short sentence will suffice the introduction should simply let people know your name and your relation to the deceased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>join a support group</td>\n",
       "      <td>make sure the shares of the shares</td>\n",
       "      <td>a support group can help you join with other people who have similar obsessive thoughts or fears a support group can offer encouragement support and friendship and can help with feelings of isolation ask your medical doctor or therapist if there are any local support groups that deal with obsessive thoughts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Summary                    Predicted Summary  \\\n",
       "0  use action verbs            go to the doll                        \n",
       "1  put on some suspenders      make sure the weather                 \n",
       "2  make some assumptions       make sure your hands                  \n",
       "3  write a brief introduction  make sure your hands and the morsel   \n",
       "4  join a support group        make sure the shares of the shares    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Text  \n",
       "0  edit your sentences so that your verbs are consistent and add color to your work experience all verbs should be in the same tenses write about previous projects in the past tense but the description of a job function or institution in the present tense all verbs should be in the active not passive voice use verbs that match or are synonymous with some verbs on the job description you are applying for                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "1  authentic lederhosen may come with suspenders but if you buy them separately try to find some that match the color of your breeches                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2  if you think you have real cause to suspect your spouse then start with the assumption that she is going to take some kind of precautions to remain undiscovered when cheating she is not going to send emails from the home computer or call from the home phone she is not going to claim to be working late and leave for a hotel rendezvous risking your calls going unanswered or being seen leaving work too early she will use normal routines and patterns that you are well used to and simply use that time to have the affair a sexual affair does not require much time or commitment the two of them meet in the parking lot hop into one car head for their room at the motel 9 for a half hour and are back in time for shopping she even comes home with purchases consistent with where they were supposed to be so if you are truly committed to finding the truth do this  \n",
       "3  if you come from a large family or if your grandparent had a lot of friends there is a chance that not everyone will know you as the grandchild keep your introduction very brief just a short sentence will suffice the introduction should simply let people know your name and your relation to the deceased                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "4  a support group can help you join with other people who have similar obsessive thoughts or fears a support group can offer encouragement support and friendship and can help with feelings of isolation ask your medical doctor or therapist if there are any local support groups that deal with obsessive thoughts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_df = pd.DataFrame({\n",
    "    'Average F1':avg_rouge_f1,\n",
    "    'Average Precision': avg_rouge_precision,\n",
    "    'Average Recalls': avg_rouge_recall}, index=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average F1</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recalls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Average F1  Average Precision  Average Recalls\n",
       " 0.09        0.13               0.08            "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
